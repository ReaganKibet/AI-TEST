# AI Creative Generation Platform

A powerful AI platform that combines Large Language Models (LLMs) and image generation capabilities to create an interactive creative generation system. This platform provides a RESTful API interface with Swagger documentation for easy integration and usage.

## ğŸŒŸ Features

- **LLM Integration**: Powered by state-of-the-art language models
- **Image Generation**: Advanced image processing and generation capabilities
- **Memory Management**: Sophisticated memory system for context-aware responses
- **Pipeline Management**: Flexible pipeline system for processing requests
- **RESTful API**: Well-documented API endpoints with Swagger UI
- **Docker Support**: Containerized deployment with Docker and Docker Compose
- **Configurable**: Extensive configuration options for customization

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+
- Docker and Docker Compose (for containerized deployment)
- CUDA-capable GPU (optional, for faster inference)

### Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd <repository-name>
```

2. Create and activate a virtual environment:
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

### Running the Application

#### Local Development

1. Start the application:
```bash
python app/main.py
```

2. Access the Swagger UI at `http://localhost:8000`

#### Docker Deployment

1. Build and run using Docker Compose:
```bash
docker-compose up --build
```

2. Access the application at `http://localhost:8000`

## ğŸ“š API Documentation

The API documentation is available through Swagger UI at the root endpoint (`/`). Key endpoints include:

- `POST /execute`: Generate creative content based on prompts
- `GET /outputs/images/<filename>`: Access generated images
- Additional endpoints for configuration and management

## ğŸ› ï¸ Project Structure

```
.
â”œâ”€â”€ app/                    # Main application directory
â”‚   â”œâ”€â”€ core/              # Core functionality
â”‚   â”œâ”€â”€ config/            # Configuration files
â”‚   â”œâ”€â”€ datastore/         # Data storage
â”‚   â”œâ”€â”€ main.py            # Main application entry point
â”‚   â”œâ”€â”€ llm_manager.py     # LLM management
â”‚   â”œâ”€â”€ image_manager.py   # Image processing
â”‚   â”œâ”€â”€ memory_manager.py  # Memory system
â”‚   â””â”€â”€ pipeline_manager.py # Pipeline management
â”œâ”€â”€ outputs/               # Generated outputs
â”œâ”€â”€ logs/                  # Application logs
â”œâ”€â”€ Dockerfile            # Docker configuration
â”œâ”€â”€ docker-compose.yml    # Docker Compose configuration
â””â”€â”€ requirements.txt      # Python dependencies
```

## âš™ï¸ Configuration

The application can be configured through various configuration files:

- `config/llm_config.json`: LLM model settings
- Environment variables for sensitive configurations
- Docker environment variables for containerized deployment

## ğŸ”§ Development

### Adding New Features

1. Create new modules in the appropriate directories
2. Update the pipeline manager for new processing steps
3. Add new API endpoints in `main.py`
4. Update Swagger documentation

### Testing

```bash
# Run tests
python -m pytest tests/
```


## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request



## ğŸ™ Acknowledgments

- OpenFabric SDK
- Hugging Face Transformers
- Other open-source libraries and tools used in this project